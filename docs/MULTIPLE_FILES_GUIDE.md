# å¤šæ–‡ä»¶åŠ è½½æŒ‡å—

å¤„ç†å¤šä¸ª Parquet æ–‡ä»¶çš„å®Œæ•´æŒ‡å—ã€‚

---

## ğŸ“Š ä¸‰ç§åœºæ™¯

### åœºæ™¯ 1: åŒæ„æ–‡ä»¶åˆå¹¶ï¼ˆConcatï¼‰

**é€‚ç”¨äº**: ç»“æ„ç›¸åŒçš„å¤šä¸ªæ–‡ä»¶
- å¤šå¹´æ•°æ®ï¼ˆ2022.parquet + 2023.parquet + ...ï¼‰
- åˆ†ç‰‡æ•°æ®ï¼ˆpart1.parquet + part2.parquet + ...ï¼‰
- åˆ†æ‰¹æ¬¡å¯¼å‡ºçš„æ•°æ®

**ç‰¹ç‚¹**: çºµå‘å †å ï¼Œå­—æ®µç›¸åŒæˆ–å…¼å®¹

---

### åœºæ™¯ 2: å¼‚æ„æ–‡ä»¶å…³è”ï¼ˆJoinï¼‰

**é€‚ç”¨äº**: ä¸åŒè¡¨æœ‰å¤–é”®å…³ç³»
- ä¸»è¡¨ + ç»´åº¦è¡¨ï¼ˆä¿å• + å®¢æˆ·ä¿¡æ¯')
- äº‹å®è¡¨ + å¤šä¸ªç»´åº¦è¡¨ï¼ˆè®¢å• + å®¢æˆ· + äº§å“ï¼‰
- é›ªèŠ±æ¨¡å‹/æ˜Ÿå‹æ¨¡å‹æ•°æ®

**ç‰¹ç‚¹**: æ¨ªå‘æ‹¼æ¥ï¼Œé€šè¿‡å…³é”®å­—æ®µå…³è”

---

### åœºæ™¯ 3: ç‹¬ç«‹æ–‡ä»¶æ‰¹é‡åŠ è½½

**é€‚ç”¨äº**: å¤šä¸ªä¸ç›¸å…³çš„æ•°æ®é›†
- ä¸åŒä¸šåŠ¡çº¿æ•°æ®ï¼ˆé”€å”® + HR + è´¢åŠ¡ï¼‰
- ä¸åŒä¸»é¢˜æ•°æ®ï¼ˆç”¨æˆ· + å•†å“ + è®¢å•ï¼‰

**ç‰¹ç‚¹**: åˆ†åˆ«åŠ è½½ï¼Œç‹¬ç«‹åˆ†æ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åœºæ™¯ 1: åˆå¹¶åŒæ„æ–‡ä»¶

```python
from src.session import DataSession

session = DataSession()

# åˆå¹¶å¤šä¸ªæ–‡ä»¶
session.load_multiple_concat(
    ['data/2022.parquet', 'data/2023.parquet', 'data/2024.parquet'],
    alias='all_years'
)

# ä½¿ç”¨
all_years_df.filter(pl.col('ä¸šåŠ¡å¹´åº¦') == '2023')
```

**ä½¿ç”¨ glob æ¨¡å¼**:

```python
# åŒ¹é…æ‰€æœ‰å¹´ä»½æ–‡ä»¶
session.load_multiple_concat(
    ['data/year_*.parquet'],
    alias='all_data'
)

# åŒ¹é…å¤šä¸ªç›®å½•
session.load_multiple_concat(
    ['old_format/*.parquet', 'new_format/*.parquet'],
    alias='combined',
    ignore_schema_errors=True  # å®¹é”™æ¨¡å¼
)
```

---

### åœºæ™¯ 2: å…³è”å¼‚æ„æ–‡ä»¶

```python
# ç®€å• join
session.load_multiple_join(
    files={
        'policy': 'data/ä¿å•.parquet',
        'customer': 'data/å®¢æˆ·.parquet'
    },
    joins=[
        {
            'left': 'policy',
            'right': 'customer',
            'on': 'å®¢æˆ·ID',
            'how': 'left'
        }
    ],
    result_alias='enriched'
)

# ä½¿ç”¨
enriched_df.select(['ä¿å•å·', 'å®¢æˆ·åç§°', 'æ€»ä¿è´¹'])
```

**å¤šè¡¨è¿ç»­ join**:

```python
session.load_multiple_join(
    files={
        'policy': 'policy.parquet',
        'customer': 'customer.parquet',
        'product': 'product.parquet',
        'agent': 'agent.parquet'
    },
    joins=[
        # join 1: policy â† customer
        {'left': 'policy', 'right': 'customer', 'on': 'å®¢æˆ·ID', 'how': 'left'},
        
        # join 2: (policy+customer) â† product
        # left ä¼šè‡ªåŠ¨ä½¿ç”¨ä¸Šä¸€æ­¥çš„ç»“æœ
        {'left': 'policy', 'right': 'product', 'on': 'äº§å“ä»£ç ', 'how': 'left'},
        
        # join 3: (policy+customer+product) â† agent
        {'left': 'policy', 'right': 'agent', 'on': 'ä»£ç†äººä»£ç ', 'how': 'left'}
    ],
    result_alias='full_data'
)
```

---

### åœºæ™¯ 3: æ‰¹é‡åŠ è½½ç‹¬ç«‹æ–‡ä»¶

```python
session.load_multiple_independent({
    'sales': 'data/é”€å”®.parquet',
    'hr': 'data/äººåŠ›.parquet',
    'finance': 'data/è´¢åŠ¡.parquet'
})

# åˆ†åˆ«ä½¿ç”¨
sales_df.head()
hr_df.filter(pl.col('éƒ¨é—¨') == 'IT')
finance_df.group_by('æœˆä»½').agg(...)
```

---

## ğŸ“‹ å®Œæ•´ API æ–‡æ¡£

### `load_multiple_concat()`

çºµå‘åˆå¹¶åŒæ„æ–‡ä»¶

**å‚æ•°**:
- `file_patterns: list[str]` - æ–‡ä»¶è·¯å¾„åˆ—è¡¨æˆ– glob æ¨¡å¼
- `alias: str` - åˆå¹¶åçš„åˆ«å
- `ignore_schema_errors: bool = False` - æ˜¯å¦å¿½ç•¥ schema ä¸åŒ¹é…

**è¿”å›**: `pl.DataFrame`

**ç¤ºä¾‹**:
```python
session.load_multiple_concat(
    ['data/*.parquet'],
    alias='all_data',
    ignore_schema_errors=True
)
```

---

### `load_multiple_join()`

å…³è”å¼‚æ„æ–‡ä»¶

**å‚æ•°**:
- `files: dict[str, str]` - {åˆ«å: æ–‡ä»¶è·¯å¾„} å­—å…¸
- `joins: list[dict]` - join é…ç½®åˆ—è¡¨
- `result_alias: str` - æœ€ç»ˆç»“æœåˆ«å

**join é…ç½®**:
- `left: str` - å·¦è¡¨åˆ«å
- `right: str` - å³è¡¨åˆ«å
- `on: str | list[str]` - è¿æ¥å­—æ®µ
- `how: str` - è¿æ¥æ–¹å¼ (`left`/`inner`/`outer`/`cross`)
- `suffix: str` - å¯é€‰ï¼Œå³è¡¨é‡ååˆ—åç¼€ï¼ˆé»˜è®¤ `_right`ï¼‰

**è¿”å›**: `pl.DataFrame`

**ç¤ºä¾‹**:
```python
session.load_multiple_join(
    files={'orders': 'orders.parquet', 'items': 'items.parquet'},
    joins=[{
        'left': 'orders',
        'right': 'items',
        'on': ['è®¢å•å·', 'å¹´ä»½'],
        'how': 'inner',
        'suffix': '_item'
    }],
    result_alias='order_details'
)
```

---

### `load_multiple_independent()`

æ‰¹é‡åŠ è½½ç‹¬ç«‹æ–‡ä»¶

**å‚æ•°**:
- `files: dict[str, str]` - {åˆ«å: æ–‡ä»¶è·¯å¾„} å­—å…¸

**è¿”å›**: `dict[str, pl.DataFrame]`

**ç¤ºä¾‹**:
```python
session.load_multiple_independent({
    'sales': 'sales.parquet',
    'hr': 'hr.parquet'
})
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. åˆå¹¶æ–‡ä»¶æ—¶çš„æ³¨æ„äº‹é¡¹

**ç¡®ä¿æ–‡ä»¶é¡ºåº**:
```python
# ä½¿ç”¨æ’åºçš„ glob
import glob
files = sorted(glob.glob('data/year_*.parquet'))
session.load_multiple_concat(files, alias='ordered')
```

**Schema ä¸ä¸€è‡´å¤„ç†**:
```python
# å¯ç”¨å®¹é”™æ¨¡å¼
session.load_multiple_concat(
    ['old/*.parquet', 'new/*.parquet'],
    alias='mixed',
    ignore_schema_errors=True  # ç¼ºå¤±å­—æ®µå¡«å…… null
)

# æ‰‹åŠ¨å¯¹é½ schemaï¼ˆæ›´å®‰å…¨ï¼‰
session.load_multiple_concat(
    ['old/*.parquet'],
    alias='old_data'
)
# åœ¨ Polars ä¸­æ‰‹åŠ¨æ·»åŠ ç¼ºå¤±åˆ—
old_data_df = old_data_df.with_columns([
    pl.lit(None).alias('æ–°å­—æ®µ')
])
```

---

### 2. Join æ—¶çš„æ³¨æ„äº‹é¡¹

**æ£€æŸ¥å…³è”è´¨é‡**:
```python
# åŠ è½½åæ£€æŸ¥
result = session.load_multiple_join(...)

# æ£€æŸ¥ join æ˜¯å¦æœ‰æ•°æ®ä¸¢å¤±
print(f"ä¿å•æ•°: {policy_df.height}")
print(f"Join å: {result.height}")

# æ£€æŸ¥ null å€¼
result.select([pl.col('*').is_null().sum()])
```

**å¤„ç†é‡ååˆ—**:
```python
# ä½¿ç”¨ suffix
session.load_multiple_join(
    files={'a': 'a.parquet', 'b': 'b.parquet'},
    joins=[{
        'left': 'a',
        'right': 'b',
        'on': 'id',
        'suffix': '_from_b'  # é‡ååˆ—ä¼šå˜æˆ name_from_b
    }],
    result_alias='joined'
)
```

---

### 3. æ€§èƒ½ä¼˜åŒ–

**å¤§æ–‡ä»¶å¤„ç†**:
```python
# åˆ†æ‰¹å¤„ç†
for year in range(2020, 2025):
    session.load_multiple_concat(
        [f'data/{year}_*.parquet'],
        alias=f'year_{year}'
    )
    # å¤„ç†å•å¹´æ•°æ®
    process(eval(f'year_{year}_df'))
```

**å†…å­˜ç®¡ç†**:
```python
# ä½¿ç”¨åæ¸…é™¤
session.load_multiple_concat(['large_*.parquet'], alias='temp')
# ... ä½¿ç”¨ temp_df ...
session.clear('temp_df')  # é‡Šæ”¾å†…å­˜
```

---

## ğŸ¯ å®é™…æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: å¤šå¹´ä¿é™©æ•°æ®åˆ†æ

```python
# 1. åˆå¹¶å¤šå¹´ä¿å•
session.load_multiple_concat(
    [f'data/policy_{y}.parquet' for y in range(2020, 2025)],
    alias='all_policies'
)

# 2. å…³è”ç»´åº¦è¡¨
session.load_multiple_join(
    files={
        'policy': 'df_all_policies',  # å¯ä»¥ä½¿ç”¨å·²åŠ è½½çš„
        'customer': 'data/customer.parquet',
        'product': 'data/product.parquet'
    },
    joins=[
        {'left': 'policy', 'right': 'customer', 'on': 'å®¢æˆ·ID', 'how': 'left'},
        {'left': 'policy', 'right': 'product', 'on': 'äº§å“ä»£ç ', 'how': 'left'}
    ],
    result_alias='enriched'
)

# 3. åˆ†æ
enriched_df.group_by(['ä¸šåŠ¡å¹´åº¦', 'äº§å“ç±»å‹']).agg([
    pl.col('æ€»ä¿è´¹').sum(),
    pl.len().alias('ä¿å•æ•°')
])
```

---

### æ¡ˆä¾‹ 2: ç”µå•†è®¢å•åˆ†æ

```python
# è®¢å•ä¸»è¡¨ + å¤šä¸ªç»´åº¦è¡¨
session.load_multiple_join(
    files={
        'orders': 'orders.parquet',
        'customers': 'customers.parquet',
        'products': 'products.parquet',
        'sellers': 'sellers.parquet'
    },
    joins=[
        {'left': 'orders', 'right': 'customers', 'on': 'customer_id', 'how': 'left'},
        {'left': 'orders', 'right': 'products', 'on': 'product_id', 'how': 'left'},
        {'left': 'orders', 'right': 'sellers', 'on': 'seller_id', 'how': 'left'}
    ],
    result_alias='full_orders'
)
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: Glob æ¨¡å¼åŒ¹é…ä¸åˆ°æ–‡ä»¶ï¼Ÿ

```python
# æ£€æŸ¥è·¯å¾„
import glob
print(glob.glob('data/*.parquet'))

# ä½¿ç”¨ç»å¯¹è·¯å¾„
import os
data_dir = os.path.abspath('data')
session.load_multiple_concat(
    [f'{data_dir}/*.parquet'],
    alias='data'
)
```

### Q2: Schema ä¸åŒ¹é…é”™è¯¯ï¼Ÿ

```python
# ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š

# æ–¹æ¡ˆ 1: å¯ç”¨å®¹é”™
session.load_multiple_concat(
    ['*.parquet'],
    alias='data',
    ignore_schema_errors=True
)

# æ–¹æ¡ˆ 2: æ‰‹åŠ¨å¯¹é½ï¼ˆæ›´å®‰å…¨ï¼‰
# å…ˆæ£€æŸ¥æ¯ä¸ªæ–‡ä»¶çš„ schema
for f in glob.glob('*.parquet'):
    df = pl.read_parquet(f)
    print(f"{f}: {df.columns}")
```

### Q3: Join åæ•°æ®é‡ä¸å¯¹ï¼Ÿ

```python
# æ£€æŸ¥å…³è”é”®æ˜¯å¦å”¯ä¸€
customer_df.group_by('å®¢æˆ·ID').len().filter(pl.col('len') > 1)

# ä½¿ç”¨ inner join çœ‹æ•°æ®åŒ¹é…æƒ…å†µ
session.load_multiple_join(
    ...,
    joins=[{'..., 'how': 'inner'}],  # åªä¿ç•™åŒ¹é…çš„
    ...
)
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [DataSession API æ–‡æ¡£](../docs/API.md)
- [Polars Join æ–‡æ¡£](https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/api/polars.DataFrame.join.html)
- [ç¤ºä¾‹ Notebook](../notebooks/examples/multiple_files_examples.ipynb)

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** æŸ¥çœ‹ç¤ºä¾‹ notebook æˆ–æ issueï¼ğŸš€
