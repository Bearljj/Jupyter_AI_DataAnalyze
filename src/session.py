"""æ•°æ®ä¼šè¯ç®¡ç†

è¿™æ˜¯æ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ï¼Œç”¨äºï¼š
1. é¿å…é‡å¤çš„æ•°æ®åŠ è½½ä»£ç 
2. å°†æ•°æ®æ³¨å…¥åˆ°å…¨å±€å‘½åç©ºé—´ï¼ŒAI å¯ä»¥ç›´æ¥ä½¿ç”¨
3. ç”Ÿæˆ AI-Friendly çš„æ•°æ®æ¦‚è§ˆ
"""

import polars as pl
from pathlib import Path
from typing import Dict, Optional
from datetime import datetime

from src.data.loaders import load_data


class DataSession:
    """
    æ•°æ®ä¼šè¯ç®¡ç†å™¨
    
    ä¸€æ¬¡åŠ è½½ï¼Œnotebook å†…å…¨å±€ä½¿ç”¨
    AI ç”Ÿæˆçš„ä»£ç å¯ä»¥ç›´æ¥å¼•ç”¨åŠ è½½çš„å˜é‡
    
    Examples:
        >>> session = DataSession()
        >>> session.load("2024_01", alias="df_jan")
        >>> # ç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨ df_janï¼Œæ— éœ€é‡å¤åŠ è½½
        >>> result = df_jan.group_by('product').agg(...)
    """
    
    def __init__(self):
        self.loaded_data: Dict[str, pl.DataFrame] = {}
        self.metadata: Dict[str, dict] = {}
    
    def load(
        self,
        dataset_id: str,
        alias: str = None,
        lazy: bool = False
    ) -> pl.DataFrame:
        """
        åŠ è½½æ•°æ®é›†åˆ°ä¼šè¯
        
        Args:
            dataset_id: æ•°æ®é›†IDæˆ–æ–‡ä»¶è·¯å¾„
            alias: å˜é‡åˆ«åï¼ˆå¦‚ "jan", "feb"ï¼‰
            lazy: æ˜¯å¦æƒ°æ€§åŠ è½½
        
        Returns:
            åŠ è½½çš„ DataFrame
        
        Examples:
            >>> session.load("2024_01", alias="df_jan")
            >>> session.load("reinsurance/2024_01.parquet", alias="jan")
        """
        # åŠ è½½æ•°æ®
        try:
            df = load_data(dataset_id, lazy=lazy)
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            raise
        
        # ç”Ÿæˆå˜é‡å
        if alias:
            var_name = alias if alias.startswith("df_") else f"df_{alias}"
        else:
            # ä»è·¯å¾„è‡ªåŠ¨ç”Ÿæˆå˜é‡å
            var_name = f"df_{Path(dataset_id).stem}"
        
        # å­˜å‚¨åˆ°ä¼šè¯
        self.loaded_data[var_name] = df
        self.metadata[var_name] = {
            'dataset_id': dataset_id,
            'loaded_at': datetime.now(),
            'lazy': lazy,
            'rows': len(df) if not lazy else "lazy",
            'cols': len(df.columns)
        }
        
        # æ³¨å…¥åˆ°å…¨å±€å‘½åç©ºé—´ï¼ˆå…³é”®ï¼ï¼‰
        try:
            import __main__
            setattr(__main__, var_name, df)
            print(f"âœ… å·²åŠ è½½: {var_name} ({dataset_id})")
        except:
            # å¦‚æœä¸åœ¨ Jupyter ç¯å¢ƒï¼Œåªå­˜å‚¨åœ¨sessionä¸­
            print(f"âœ… å·²åŠ è½½åˆ°ä¼šè¯: {var_name}")
        
        return df
    
    def get(self, var_name: str) -> Optional[pl.DataFrame]:
        """
        è·å–å·²åŠ è½½çš„æ•°æ®
        
        Args:
            var_name: å˜é‡å
        
        Returns:
            DataFrame æˆ– None
        """
        return self.loaded_data.get(var_name)
    
    def add_computed_columns(
        self,
        var_name: str,
        computed_columns: dict,
        new_alias: str = None
    ) -> pl.DataFrame:
        """
        ä¸ºå·²åŠ è½½çš„æ•°æ®æ·»åŠ è®¡ç®—åˆ—
        
        Args:
            var_name: å·²åŠ è½½æ•°æ®çš„å˜é‡å
            computed_columns: {åˆ—å: è®¡ç®—è¡¨è¾¾å¼} å­—å…¸
            new_alias: æ–°å˜é‡åï¼ˆNone åˆ™åŸåœ°ä¿®æ”¹ï¼‰
        
        Returns:
            æ·»åŠ è®¡ç®—åˆ—åçš„ DataFrame
        
        Examples:
            # æ·»åŠ ä¿è´¹åŒºé—´
            session.add_computed_columns(
                'df_insurance',
                {
                    'ä¿è´¹åŒºé—´': pl.when(pl.col('æ€»ä¿è´¹') >= 100000)
                                  .then(pl.lit('å¤§é¢'))
                                  .otherwise(pl.lit('å°é¢')),
                    'å¹´ä»½': pl.col('ä¿é™©èµ·æœŸ').str.slice(0, 4)
                }
            )
            
            # æˆ–åˆ›å»ºæ–°æ•°æ®é›†
            session.add_computed_columns(
                'df_raw',
                {'ä¿è´¹åŒºé—´': ...},
                new_alias='enriched'
            )
        """
        # è·å–åŸæ•°æ®
        df = self.loaded_data.get(var_name)
        if df is None:
            raise ValueError(f"æ•°æ® '{var_name}' ä¸å­˜åœ¨")
        
        # æ·»åŠ è®¡ç®—åˆ—
        expressions = list(computed_columns.values())
        df_new = df.with_columns(expressions)
        
        # ç¡®å®šç›®æ ‡å˜é‡å
        target_var = new_alias if new_alias else var_name
        if target_var != var_name and not target_var.startswith('df_'):
            target_var = f"df_{target_var}"
        
        # æ›´æ–°ä¼šè¯
        self.loaded_data[target_var] = df_new
        self.metadata[target_var] = {
            'dataset_id': f"computed({var_name})",
            'loaded_at': datetime.now(),
            'lazy': False,
            'rows': len(df_new),
            'cols': len(df_new.columns),
            'computed_columns': list(computed_columns.keys())
        }
        
        # æ³¨å…¥åˆ°å…¨å±€
        try:
            import __main__
            setattr(__main__, target_var, df_new)
        except:
            pass
        
        print(f"âœ… å·²æ·»åŠ  {len(computed_columns)} ä¸ªè®¡ç®—åˆ—: {list(computed_columns.keys())}")
        if new_alias:
            print(f"ğŸ’¡ æ–°å˜é‡: {target_var}")
        else:
            print(f"ğŸ’¡ å·²æ›´æ–°: {target_var}")
        
        return df_new
    
    def list_loaded(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰å·²åŠ è½½çš„æ•°æ®é›†"""
        return list(self.loaded_data.keys())
    
    def summary(self) -> None:
        """æ˜¾ç¤ºä¼šè¯æ‘˜è¦"""
        if not self.loaded_data:
            print("âš ï¸  æ²¡æœ‰åŠ è½½ä»»ä½•æ•°æ®é›†")
            return
        
        print("âœ… æ•°æ®ä¼šè¯å·²åˆå§‹åŒ–\n")
        print("å·²åŠ è½½æ•°æ®é›†ï¼š\n")
        
        total_memory = 0
        for i, (var_name, df) in enumerate(self.loaded_data.items(), 1):
            meta = self.metadata[var_name]
            
            # ä¼°ç®—å†…å­˜å ç”¨
            if not meta['lazy']:
                try:
                    memory_mb = df.estimated_size() / 1024 / 1024
                    total_memory += memory_mb
                    memory_str = f"{memory_mb:.1f} MB"
                except:
                    memory_str = "unknown"
            else:
                memory_str = "lazy (æœªåŠ è½½åˆ°å†…å­˜)"
            
            rows = meta['rows']
            rows_str = f"{rows:,}" if isinstance(rows, int) else rows
            
            print(f"  {i}. {var_name} ({meta['dataset_id']})")
            print(f"     - {rows_str} è¡Œ Ã— {meta['cols']} åˆ—")
            print(f"     - å†…å­˜: {memory_str}")
            print()
        
        if total_memory > 0:
            print(f"æ€»å†…å­˜å ç”¨: {total_memory:.1f} MB\n")
        
        print(f"ğŸ’¡ AI æç¤ºï¼šç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›å˜é‡")
        print(f"   {', '.join(self.loaded_data.keys())}\n")
    
    def get_ai_context(self) -> str:
        """
        ç”Ÿæˆå½“å‰ä¼šè¯çš„ AI Context
        
        Returns:
            åŒ…å«æ‰€æœ‰å·²åŠ è½½æ•°æ®çš„ AI Contextï¼ˆå¯ç›´æ¥å¤åˆ¶ç»™ AIï¼‰
        
        Examples:
            >>> session.load("2024_01", alias="jan")
            >>> print(session.get_ai_context())
            >>> # å¤åˆ¶è¾“å‡ºç»™ AI
        """
        if not self.loaded_data:
            return "âš ï¸  æ²¡æœ‰åŠ è½½ä»»ä½•æ•°æ®"
        
        lines = [
            "# ğŸ“Š å½“å‰æ•°æ®ä¼šè¯",
            "",
            "å·²åŠ è½½çš„æ•°æ®é›†ï¼š",
            ""
        ]
        
        for var_name, df in self.loaded_data.items():
            meta = self.metadata[var_name]
            
            rows = meta['rows']
            rows_str = f"{rows:,}" if isinstance(rows, int) else rows
            
            lines.append(f"## `{var_name}` ({meta['dataset_id']})")
            lines.append(f"**æ•°æ®é‡ï¼š** {rows_str} è¡Œ Ã— {meta['cols']} åˆ—")
            lines.append("")
            
            # åˆ—ä¿¡æ¯
            lines.append("**å­—æ®µï¼š**")
            for col in df.columns:
                dtype = str(df[col].dtype)
                lines.append(f"- `{col}` ({dtype})")
            
            lines.append("")
            lines.append("**ä½¿ç”¨ç¤ºä¾‹ï¼š**")
            lines.append("```python")
            lines.append(f"# ç›´æ¥ä½¿ç”¨å˜é‡ {var_name}")
            lines.append(f"result = {var_name}.group_by('...').agg(...)")
            lines.append(f"filtered = {var_name}.filter(pl.col('...') > 100)")
            lines.append("```")
            lines.append("")
        
        lines.append("---")
        lines.append("")
        lines.append("ğŸ’¡ **é‡è¦ï¼š** æ‰€æœ‰è¿™äº›å˜é‡éƒ½å·²åœ¨ Jupyter ç¯å¢ƒä¸­å¯ç”¨")
        lines.append("   ä½ ç”Ÿæˆçš„ä»£ç å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒä»¬ï¼Œæ— éœ€å†æ¬¡åŠ è½½")
        
        return "\n".join(lines)
    
    def load_multiple_concat(
        self,
        file_patterns: list[str],
        alias: str,
        ignore_schema_errors: bool = False,
        from_project_root: bool = True  # æ–°å¢å‚æ•°
    ) -> pl.DataFrame:
        """
        åœºæ™¯1: åŠ è½½å¤šä¸ªåŒæ„æ–‡ä»¶å¹¶çºµå‘åˆå¹¶
        
        é€‚ç”¨äºï¼šç»“æ„ç›¸åŒçš„å¤šä¸ªæ–‡ä»¶ï¼ˆå¦‚å¤šå¹´æ•°æ®ã€åˆ†ç‰‡æ•°æ®ï¼‰
        
        Args:
            file_patterns: æ–‡ä»¶è·¯å¾„åˆ—è¡¨æˆ– glob æ¨¡å¼
            alias: åˆå¹¶åçš„åˆ«å
            ignore_schema_errors: æ˜¯å¦å¿½ç•¥schemaä¸åŒ¹é…ï¼ˆä¼šå¡«å……nullï¼‰
            from_project_root: æ˜¯å¦ä»é¡¹ç›®æ ¹ç›®å½•å¼€å§‹ï¼ˆé»˜è®¤ Trueï¼‰
        
        Returns:
            åˆå¹¶åçš„ DataFrame
        
        Examples:
            # ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ï¼ˆé»˜è®¤ï¼‰
            session.load_multiple_concat(
                ['data/processed/2022.parquet', 'data/processed/2023.parquet'],
                alias='all_years'
            )
            
            # ä½¿ç”¨ glob æ¨¡å¼
            session.load_multiple_concat(
                ['data/processed/year_*.parquet'],
                alias='all_years'
            )
            
            # ä»å½“å‰ç›®å½•åŠ è½½
            session.load_multiple_concat(
                ['./local/*.parquet'],
                alias='local_data',
                from_project_root=False
            )
        """
        import glob
        import os
        
        # è‡ªåŠ¨æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        def find_project_root():
            """å‘ä¸ŠæŸ¥æ‰¾åŒ…å« src/ ç›®å½•çš„é¡¹ç›®æ ¹ç›®å½•"""
            current = os.getcwd()
            while current != '/':
                if os.path.exists(os.path.join(current, 'src')) and \
                   os.path.exists(os.path.join(current, 'data')):
                    return current
                current = os.path.dirname(current)
            return os.getcwd()  # æ‰¾ä¸åˆ°å°±è¿”å›å½“å‰ç›®å½•
        
        def search_file_in_common_locations(filename, root_dir):
            """åœ¨å¸¸è§æ•°æ®ç›®å½•ä¸­æœç´¢æ–‡ä»¶"""
            # å¸¸è§çš„æ•°æ®ç›®å½•ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            common_dirs = [
                'data/processed',
                'data/raw',
                'data',
                'data/external',
                'data/interim'
            ]
            
            for dir_path in common_dirs:
                full_path = os.path.join(root_dir, dir_path, filename)
                # æ”¯æŒ glob æ¨¡å¼
                if '*' in filename or '?' in filename:
                    matches = glob.glob(full_path)
                    if matches:
                        return matches[0]  # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…
                elif os.path.exists(full_path):
                    return full_path
            
            return None  # æ‰¾ä¸åˆ°
        
        # è§£æè·¯å¾„
        if from_project_root:
            root_dir = find_project_root()
            resolved_patterns = []
            
            for pattern in file_patterns:
                if os.path.isabs(pattern):
                    # å·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
                    resolved_patterns.append(pattern)
                elif '/' in pattern or '\\' in pattern:
                    # åŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼Œè§†ä¸ºç›¸å¯¹è·¯å¾„
                    resolved_patterns.append(os.path.join(root_dir, pattern))
                else:
                    # åªæœ‰æ–‡ä»¶åï¼Œè‡ªåŠ¨æœç´¢
                    found = search_file_in_common_locations(pattern, root_dir)
                    if found:
                        print(f"  ğŸ“ è‡ªåŠ¨æ‰¾åˆ°: {pattern} â†’ {os.path.relpath(found, root_dir)}")
                        resolved_patterns.append(found)
                    else:
                        # æ‰¾ä¸åˆ°ï¼Œé»˜è®¤æ”¾åœ¨ data/processed
                        default_path = os.path.join(root_dir, 'data/processed', pattern)
                        resolved_patterns.append(default_path)
                        print(f"  âš ï¸  æœªæ‰¾åˆ° {pattern}ï¼Œå°è¯•é»˜è®¤è·¯å¾„: data/processed/{pattern}")
            
            print(f"ğŸ“‚ ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½: {root_dir}")
        else:
            resolved_patterns = file_patterns
        
        # å±•å¼€ glob æ¨¡å¼
        files = []
        for pattern in resolved_patterns:
            if '*' in pattern or '?' in pattern:
                matched = glob.glob(pattern)
                files.extend(sorted(matched))  # æ’åºä¿è¯é¡ºåº
            else:
                files.append(pattern)
        
        if not files:
            raise ValueError(f"æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶: {file_patterns}\nè§£æåçš„è·¯å¾„: {resolved_patterns}")
        
        print(f"ğŸ“‚ å‘ç° {len(files)} ä¸ªæ–‡ä»¶å‡†å¤‡åˆå¹¶")
        
        # åŠ è½½æ‰€æœ‰æ–‡ä»¶
        dfs = []
        total_rows = 0
        for file in files:
            try:
                df = pl.read_parquet(file)
                dfs.append(df)
                total_rows += df.height
                print(f"  âœ… {os.path.basename(file)}: {df.height:,} è¡Œ Ã— {df.width} åˆ—")
            except Exception as e:
                print(f"  âŒ {os.path.basename(file)}: {e}")
                if not ignore_schema_errors:
                    raise
        
        if not dfs:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡ä»¶")
        
        # çºµå‘åˆå¹¶
        print(f"\nğŸ”— åˆå¹¶ä¸­...")
        combined = pl.concat(
            dfs,
            how='vertical_relaxed' if ignore_schema_errors else 'vertical'
        )
        
        # ç”Ÿæˆå˜é‡å
        var_name = alias if alias.startswith("df_") else f"df_{alias}"
        
        # å­˜å‚¨
        self.loaded_data[var_name] = combined
        self.metadata[var_name] = {
            'dataset_id': f"concat({len(files)} files)",
            'loaded_at': datetime.now(),
            'lazy': False,
            'rows': combined.height,
            'cols': combined.width,
            'source_files': files
        }
        
        # æ³¨å…¥åˆ°å…¨å±€
        try:
            import __main__
            setattr(__main__, var_name, combined)
        except:
            pass
        
        print(f"\nâœ… åˆå¹¶å®Œæˆ: {var_name}")
        print(f"   æ€»è®¡: {combined.height:,} è¡Œ Ã— {combined.width} åˆ—")
        print(f"   æ¥æº: {len(files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ’¡ ä½¿ç”¨å˜é‡: {var_name}")
        
        return combined
    
    def load_multiple_join(
        self,
        files: dict[str, str],
        joins: list[dict],
        result_alias: str,
        from_project_root: bool = True  # æ–°å¢å‚æ•°
    ) -> pl.DataFrame:
        """
        åœºæ™¯2: åŠ è½½å¤šä¸ªå¼‚æ„æ–‡ä»¶å¹¶æ ¹æ®å…³è”å…³ç³»join
        
        é€‚ç”¨äºï¼šä¸åŒè¡¨æœ‰å¤–é”®å…³ç³»ï¼ˆå¦‚è®¢å•-å®¢æˆ·-äº§å“ï¼‰
        
        Args:
            files: {åˆ«å: æ–‡ä»¶è·¯å¾„} å­—å…¸
            joins: joiné…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«ï¼š
                - left: å·¦è¡¨åˆ«å
                - right: å³è¡¨åˆ«å
                - on: è¿æ¥å­—æ®µï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
                - how: è¿æ¥æ–¹å¼ (left/inner/outer/cross)
                - suffix: å¯é€‰ï¼Œå³è¡¨é‡ååˆ—åç¼€
            result_alias: æœ€ç»ˆç»“æœçš„åˆ«å
            from_project_root: æ˜¯å¦ä»é¡¹ç›®æ ¹ç›®å½•å¼€å§‹ï¼ˆé»˜è®¤ Trueï¼‰
        
        Returns:
            joinåçš„ DataFrame
        
        Examples:
            session.load_multiple_join(
                files={
                    'policy': 'data/processed/policy.parquet',
                    'customer': 'data/processed/customer.parquet',
                    'product': 'data/processed/product.parquet'
                },
                joins=[
                    {'left': 'policy', 'right': 'customer', 'on': 'å®¢æˆ·ID', 'how': 'left'},
                    {'left': 'policy', 'right': 'product', 'on': 'äº§å“ä»£ç ', 'how': 'left'}
                ],
                result_alias='enriched'
            )
        """
        import os
        import glob
        
        # è‡ªåŠ¨æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        def find_project_root():
            current = os.getcwd()
            while current != '/':
                if os.path.exists(os.path.join(current, 'src')) and \
                   os.path.exists(os.path.join(current, 'data')):
                    return current
                current = os.path.dirname(current)
            return os.getcwd()
        
        def search_file_in_common_locations(filename, root_dir):
            """åœ¨å¸¸è§æ•°æ®ç›®å½•ä¸­æœç´¢æ–‡ä»¶"""
            common_dirs = [
                'data/processed',
                'data/raw',
                'data',
                'data/external',
                'data/interim'
            ]
            
            for dir_path in common_dirs:
                full_path = os.path.join(root_dir, dir_path, filename)
                if os.path.exists(full_path):
                    return full_path
            
            return None
        
        # è§£ææ–‡ä»¶è·¯å¾„
        if from_project_root:
            root_dir = find_project_root()
            resolved_files = {}
            
            for alias, filepath in files.items():
                if os.path.isabs(filepath):
                    # ç»å¯¹è·¯å¾„
                    resolved_files[alias] = filepath
                elif '/' in filepath or '\\' in filepath:
                    # ç›¸å¯¹è·¯å¾„
                    resolved_files[alias] = os.path.join(root_dir, filepath)
                else:
                    # åªæœ‰æ–‡ä»¶åï¼Œè‡ªåŠ¨æœç´¢
                    found = search_file_in_common_locations(filepath, root_dir)
                    if found:
                        print(f"  ğŸ“ è‡ªåŠ¨æ‰¾åˆ° {alias}: {filepath} â†’ {os.path.relpath(found, root_dir)}")
                        resolved_files[alias] = found
                    else:
                        # é»˜è®¤ data/processed
                        default_path = os.path.join(root_dir, 'data/processed', filepath)
                        resolved_files[alias] = default_path
                        print(f"  âš ï¸  æœªæ‰¾åˆ° {alias} ({filepath})ï¼Œå°è¯•: data/processed/{filepath}")
            
            print(f"ğŸ“‚ ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½: {root_dir}")
        else:
            resolved_files = files
        
        print(f"ğŸ“‚ åŠ è½½ {len(resolved_files)} ä¸ªæ–‡ä»¶")
        
        # 1. åŠ è½½æ‰€æœ‰æ–‡ä»¶
        loaded = {}
        for alias, filepath in resolved_files.items():
            try:
                df = pl.read_parquet(filepath)
                loaded[alias] = df
                print(f"  âœ… {alias}: {df.height:,} è¡Œ Ã— {df.width} åˆ—")
            except Exception as e:
                raise ValueError(f"åŠ è½½å¤±è´¥ {alias} ({filepath}): {e}")
        
        # 2. éªŒè¯ join é…ç½®
        for i, jc in enumerate(joins):
            if 'left' not in jc or 'right' not in jc or 'on' not in jc:
                raise ValueError(f"Join {i+1} é…ç½®ä¸å®Œæ•´: {jc}")
        
        # 3. æ‰§è¡Œè¿ç»­join
        print(f"\nğŸ”— æ‰§è¡Œ {len(joins)} ä¸ªJoinæ“ä½œ")
        result = None
        
        for i, join_config in enumerate(joins, 1):
            left_alias = join_config['left']
            right_alias = join_config['right']
            on = join_config['on']
            how = join_config.get('how', 'left')
            suffix = join_config.get('suffix', '_right')
            
            # ç¡®å®šå·¦è¡¨
            if result is None:
                if left_alias not in loaded:
                    raise ValueError(f"å·¦è¡¨ '{left_alias}' ä¸å­˜åœ¨")
                left_df = loaded[left_alias]
            else:
                # ä½¿ç”¨ä¸Šä¸€æ­¥çš„ç»“æœ
                left_df = result
            
            # ç¡®å®šå³è¡¨
            if right_alias not in loaded:
                raise ValueError(f"å³è¡¨ '{right_alias}' ä¸å­˜åœ¨")
            right_df = loaded[right_alias]
            
            # æ‰§è¡Œjoin
            result = left_df.join(right_df, on=on, how=how, suffix=suffix)
            
            print(f"  Join {i}: {left_alias} â† {right_alias}")
            print(f"    è¿æ¥å­—æ®µ: {on}")
            print(f"    è¿æ¥æ–¹å¼: {how}")
            print(f"    ç»“æœ: {result.height:,} è¡Œ Ã— {result.width} åˆ—")
        
        # 4. å­˜å‚¨ç»“æœ
        var_name = result_alias if result_alias.startswith("df_") else f"df_{result_alias}"
        
        self.loaded_data[var_name] = result
        self.metadata[var_name] = {
            'dataset_id': f"join({', '.join(resolved_files.keys())})",
            'loaded_at': datetime.now(),
            'lazy': False,
            'rows': result.height,
            'cols': result.width,
            'source_files': list(resolved_files.values()),
            'joins': joins
        }
        
        # æ³¨å…¥åˆ°å…¨å±€
        try:
            import __main__
            setattr(__main__, var_name, result)
        except:
            pass
        
        print(f"\nâœ… Join å®Œæˆ: {var_name}")
        print(f"   {result.height:,} è¡Œ Ã— {result.width} åˆ—")
        print(f"ğŸ’¡ ä½¿ç”¨å˜é‡: {var_name}")
        
        return result
    
    def load_multiple_independent(
        self,
        files: dict[str, str]
    ) -> dict[str, pl.DataFrame]:
        """
        åœºæ™¯3: æ‰¹é‡åŠ è½½å¤šä¸ªç‹¬ç«‹æ–‡ä»¶
        
        é€‚ç”¨äºï¼šå¤šä¸ªä¸ç›¸å…³çš„æ•°æ®é›†ï¼ˆå¦‚é”€å”®ã€HRã€è´¢åŠ¡ï¼‰
        
        Args:
            files: {åˆ«å: æ–‡ä»¶è·¯å¾„} å­—å…¸
        
        Returns:
            {åˆ«å: DataFrame} å­—å…¸
        
        Examples:
            session.load_multiple_independent({
                'sales': 'sales.parquet',
                'hr': 'hr.parquet',
                'finance': 'finance.parquet'
            })
        """
        print(f"ğŸ“‚ æ‰¹é‡åŠ è½½ {len(files)} ä¸ªç‹¬ç«‹æ–‡ä»¶")
        
        loaded = {}
        for alias, filepath in files.items():
            try:
                # ä½¿ç”¨å·²æœ‰çš„ load æ–¹æ³•
                df = self.load(filepath, alias=alias)
                loaded[alias] = df
            except Exception as e:
                print(f"  âŒ {alias}: {e}")
        
        print(f"\nâœ… å·²åŠ è½½ {len(loaded)}/{len(files)} ä¸ªæ•°æ®é›†")
        
        return loaded
    
    def clear(self, var_name: str = None) -> None:
        """
        æ¸…é™¤åŠ è½½çš„æ•°æ®ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰
        
        Args:
            var_name: å˜é‡åï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰
        """
        if var_name:
            if var_name in self.loaded_data:
                del self.loaded_data[var_name]
                del self.metadata[var_name]
                print(f"âœ… å·²æ¸…é™¤: {var_name}")
            else:
                print(f"âš ï¸  å˜é‡ä¸å­˜åœ¨: {var_name}")
        else:
            count = len(self.loaded_data)
            self.loaded_data.clear()
            self.metadata.clear()
            print(f"âœ… å·²æ¸…é™¤æ‰€æœ‰æ•°æ® ({count} ä¸ª)")
